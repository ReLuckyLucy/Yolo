import gradio as gr
import cv2
import tempfile
from ultralytics import YOLO, YOLOE
import os
import numpy as np
import time
import json
from datetime import datetime

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
os.makedirs("results", exist_ok=True)

# æ˜ å°„æ¨¡å‹åç§°å’Œæ¨¡å‹æ–‡ä»¶è·¯å¾„
model_mapping = {
    # æ™®é€šYOLOæ¨¡å‹ - ç”¨äºå›¾ç‰‡/è§†é¢‘æ¨¡å¼
    "YOLOv11n": "model/yolo11n.pt",
    "YOLOv11m": "model/yolo11m.pt",
    # YOLOEæ¨¡å‹ - ç”¨äºåˆ†å‰²æ¨¡å¼
    "YOLOev11m-pf": "model/yoloe-11m-seg-pf.pt",
    # å•ç‹¬åˆ†å‰²æ¨¡å¼ç”¨çš„æ¨¡å‹
    "YOLOev11m": "model/yoloe-11m-seg.pt",
}

# YOLOEå¸¸è§ç±»åˆ«åç§° (ä»COCOæ•°æ®é›†)
COMMON_CLASSES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
    "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse",
    "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie",
    "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove",
    "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon",
    "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut",
    "cake", "chair", "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse"
]

# ä¸ºä¸åŒæ¨¡å¼å‡†å¤‡ä¸åŒçš„æ¨¡å‹é€‰é¡¹
yolo_models = ["YOLOv11n", "YOLOv11m"]
yoloe_pf_models = ["YOLOev11m-pf"]
yoloe_seg_models = ["YOLOev11m"]

def save_results_to_json(results, output_path, mode="detection"):
    """ä¿å­˜æ£€æµ‹ç»“æœåˆ°JSONæ–‡ä»¶"""
    detection_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "mode": mode,
        "detections": []
    }
    
    try:
        for result in results:
            if not hasattr(result, 'boxes') or result.boxes is None:
                continue
                
            boxes = result.boxes
            for i, box in enumerate(boxes):
                try:
                    # å®‰å…¨åœ°è·å–ç±»åˆ«ç´¢å¼•å’Œåç§°
                    class_idx = int(box.cls[0]) if len(box.cls) > 0 else 0
                    class_name = result.names[class_idx] if class_idx in result.names else "unknown"
                    
                    # å®‰å…¨åœ°è·å–ç½®ä¿¡åº¦
                    confidence = float(box.conf[0]) if len(box.conf) > 0 else 0.0
                    
                    # å®‰å…¨åœ°è·å–è¾¹ç•Œæ¡†åæ ‡
                    if hasattr(box, 'xyxy') and box.xyxy is not None and len(box.xyxy) > 0:
                        xyxy = box.xyxy[0]
                        x1 = float(xyxy[0]) if len(xyxy) > 0 else 0.0
                        y1 = float(xyxy[1]) if len(xyxy) > 1 else 0.0
                        x2 = float(xyxy[2]) if len(xyxy) > 2 else 0.0
                        y2 = float(xyxy[3]) if len(xyxy) > 3 else 0.0
                    else:
                        x1, y1, x2, y2 = 0.0, 0.0, 0.0, 0.0
                    
                    detection = {
                        "class": class_name,
                        "confidence": confidence,
                        "bbox": {
                            "x1": x1,
                            "y1": y1,
                            "x2": x2,
                            "y2": y2
                        }
                    }
                    
                    # å®‰å…¨åœ°æ·»åŠ åˆ†å‰²æ©ç ä¿¡æ¯
                    if hasattr(result, "masks") and result.masks is not None and i < len(result.masks):
                        try:
                            mask = result.masks[i]
                            mask_data = {}
                            
                            # æ·»åŠ å½¢çŠ¶ä¿¡æ¯
                            if hasattr(mask, 'shape'):
                                # å°†numpy shapeè½¬ä¸ºæ™®é€šåˆ—è¡¨
                                shape = [int(s) for s in mask.shape]
                                mask_data["shape"] = shape
                            
                            # æ·»åŠ åƒç´ åæ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                            if hasattr(mask, 'xy') and mask.xy is not None and len(mask.xy) > i:
                                xy_points = mask.xy[i]
                                if xy_points is not None:
                                    # å°†numpyæ•°ç»„è½¬ä¸ºæ™®é€šåˆ—è¡¨ï¼Œç¡®ä¿æ‰€æœ‰å€¼æ˜¯åŸç”ŸPythonç±»å‹
                                    mask_data["xy"] = [[float(x), float(y)] for x, y in xy_points.tolist()]
                            
                            # æ·»åŠ å½’ä¸€åŒ–åæ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                            if hasattr(mask, 'xyn') and mask.xyn is not None and len(mask.xyn) > i:
                                xyn_points = mask.xyn[i]
                                if xyn_points is not None:
                                    # å°†numpyæ•°ç»„è½¬ä¸ºæ™®é€šåˆ—è¡¨ï¼Œç¡®ä¿æ‰€æœ‰å€¼æ˜¯åŸç”ŸPythonç±»å‹
                                    mask_data["xyn"] = [[float(x), float(y)] for x, y in xyn_points.tolist()]
                            
                            if mask_data:  # åªæœ‰å½“æœ‰æ©ç æ•°æ®æ—¶æ‰æ·»åŠ 
                                detection["mask"] = mask_data
                        except Exception as mask_error:
                            print(f"å¤„ç†æ©ç æ•°æ®æ—¶å‡ºé”™: {str(mask_error)}")
                            # åœ¨å‡ºé”™æ—¶æ·»åŠ ç®€åŒ–çš„æ©ç ä¿¡æ¯
                            detection["mask"] = {"error": str(mask_error)}
                    
                    detection_data["detections"].append(detection)
                except Exception as box_error:
                    print(f"å¤„ç†æ£€æµ‹æ¡†æ—¶å‡ºé”™: {str(box_error)}")
                    # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–æ£€æµ‹æ¡†
    except Exception as e:
        print(f"å¤„ç†æ£€æµ‹ç»“æœæ—¶å‡ºé”™: {str(e)}")
        # æ·»åŠ é”™è¯¯ä¿¡æ¯åˆ°JSON
        detection_data["error"] = str(e)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
    except Exception as dir_error:
        print(f"åˆ›å»ºç›®å½•æ—¶å‡ºé”™: {str(dir_error)}")
        # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä½œä¸ºå¤‡é€‰
        output_path = os.path.join(tempfile.gettempdir(), os.path.basename(output_path))
    
    # ä¿å­˜JSONæ–‡ä»¶
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(detection_data, f, ensure_ascii=False, indent=2)
        return output_path
    except Exception as json_error:
        print(f"ä¿å­˜JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(json_error)}")
        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„é”™è¯¯JSON
        error_path = os.path.join(tempfile.gettempdir(), f"error_{int(time.time())}.json")
        with open(error_path, 'w', encoding='utf-8') as f:
            json.dump({"error": str(json_error)}, f)
        return error_path

def yolo_inference(image, video, model_name, image_size, conf_threshold):
    model_path = model_mapping[model_name]
    try:
        model = YOLO(model_path)
        
        if image is not None:
            results = model.predict(source=image, imgsz=image_size, conf=conf_threshold)
            if len(results) == 0:
                # å¤„ç†ç©ºç»“æœ
                empty_image = np.zeros((400, 600, 3), dtype=np.uint8)
                cv2.putText(empty_image, "æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“", (50, 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # ä¿å­˜ç©ºç»“æœ
                json_path = os.path.join("results", f"detection_empty_{int(time.time())}.json")
                save_results_to_json([], json_path, "image_detection")
                
                return empty_image, None, json_path
            
            annotated_image = results[0].plot()
            
            # ä¿å­˜æ£€æµ‹ç»“æœåˆ°JSON
            json_path = os.path.join("results", f"detection_{int(time.time())}.json")
            save_results_to_json(results, json_path, "image_detection")
            
            return annotated_image[:, :, ::-1], None, json_path
        elif video is not None:
            try:
                video_path = tempfile.mktemp(suffix=".mp4")
                with open(video_path, "wb") as f:
                    with open(video, "rb") as g:
                        f.write(g.read())

                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    raise Exception(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
                    
                fps = cap.get(cv2.CAP_PROP_FPS)
                frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

                output_video_path = tempfile.mktemp(suffix=".mp4")
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))
                
                all_results = []
                frame_count = 0

                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break

                    frame_results = model.predict(source=frame, imgsz=image_size, conf=conf_threshold)
                    annotated_frame = frame_results[0].plot()
                    out.write(annotated_frame)
                    
                    # åªä¿å­˜æœ‰æ£€æµ‹ç»“æœçš„å¸§
                    if len(frame_results[0].boxes) > 0:
                        all_results.extend(frame_results)
                    frame_count += 1

                cap.release()
                out.release()

                # ä¿å­˜è§†é¢‘æ£€æµ‹ç»“æœåˆ°JSON
                json_path = os.path.join("results", f"video_detection_{int(time.time())}.json")
                save_results_to_json(all_results, json_path, "video_detection")
                
                return None, output_video_path, json_path
            except Exception as video_error:
                # å¤„ç†è§†é¢‘å¤„ç†é”™è¯¯
                print(f"è§†é¢‘å¤„ç†é”™è¯¯: {str(video_error)}")
                error_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
                cv2.putText(error_img, f"è§†é¢‘å¤„ç†é”™è¯¯: {str(video_error)}", (20, 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°JSON
                json_path = os.path.join("results", f"video_error_{int(time.time())}.json")
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump({"error": str(video_error)}, f, ensure_ascii=False, indent=2)
                
                return error_img, None, json_path
    except Exception as e:
        # å¤„ç†æ¨¡å‹åŠ è½½æˆ–é¢„æµ‹é”™è¯¯
        print(f"YOLOé”™è¯¯: {str(e)}")
        error_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
        cv2.putText(error_img, f"YOLOé”™è¯¯: {str(e)}", (20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°JSON
        json_path = os.path.join("results", f"error_{int(time.time())}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({"error": str(e)}, f, ensure_ascii=False, indent=2)
        
        return error_img, None, json_path

def yoloe_segmentation(image, model_name, image_size, conf_threshold, classes=None):
    """æ™®é€šåˆ†å‰²æ¨¡å¼ - ä½¿ç”¨YOLOev11-pfæ¨¡å‹"""
    model_path = model_mapping[model_name]
    
    try:
        model = YOLOE(model_path)
        
        # æ‰§è¡Œé¢„æµ‹
        results = model.predict(source=image, imgsz=image_size, conf=conf_threshold)
        
        if len(results) == 0 or len(results[0].boxes) == 0:
            # å¤„ç†ç©ºç»“æœ
            empty_image = np.zeros((400, 600, 3), dtype=np.uint8)
            cv2.putText(empty_image, "æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“", (50, 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # ä¿å­˜ç©ºç»“æœ
            json_path = os.path.join("results", f"segmentation_empty_{int(time.time())}.json")
            save_results_to_json([], json_path, "segmentation")
            
            return empty_image, json_path
            
        annotated_image = results[0].plot()
        
        # ä¿å­˜åˆ†å‰²ç»“æœåˆ°JSON
        json_path = os.path.join("results", f"segmentation_{int(time.time())}.json")
        save_results_to_json(results, json_path, "segmentation")
        
        return annotated_image[:, :, ::-1], json_path
    
    except Exception as e:
        # å¤„ç†é”™è¯¯
        print(f"åˆ†å‰²é”™è¯¯: {str(e)}")
        error_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
        cv2.putText(error_img, f"åˆ†å‰²é”™è¯¯: {str(e)}", (20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°JSON
        json_path = os.path.join("results", f"segmentation_error_{int(time.time())}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({"error": str(e)}, f, ensure_ascii=False, indent=2)
        
        return error_img, json_path

def yoloe_separate_segmentation(image, model_name, image_size, conf_threshold, classes):
    """å•ç‹¬åˆ†å‰²æ¨¡å¼ - ä½¿ç”¨YOLOev11æ¨¡å‹ï¼Œä½¿ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œå¼€æ”¾è¯æ±‡æ£€æµ‹å’Œåˆ†å‰²"""
    model_path = model_mapping[model_name]
    
    try:
        print(f"æ­£åœ¨åŠ è½½YOLOEæ¨¡å‹: {model_name}ï¼Œè·¯å¾„: {model_path}")
        model = YOLOE(model_path)
        print(f"YOLOEæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        if not classes or not classes.strip():
            class_list = ["person", "car", "dog", "cat", "chair", "table", "bottle"]
            debug_info = "ä½¿ç”¨é»˜è®¤ç±»åˆ«: " + ", ".join(class_list)
        else:
            class_list = [c.strip() for c in classes.split(',') if c.strip()]
            if not class_list:
                class_list = ["person", "car", "dog", "cat"]
            debug_info = "ä½¿ç”¨æ–‡æœ¬æç¤ºæ£€æµ‹ç±»åˆ«: " + ", ".join(class_list)
        
        text_embeddings = model.get_text_pe(class_list)
        model.set_classes(class_list, text_embeddings)
        
        actual_conf = min(conf_threshold, 0.15)
        results = model.predict(source=image, imgsz=image_size, conf=actual_conf, verbose=True)
        
        if len(results[0].boxes) == 0:
            debug_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
            y_pos = 30
            cv2.putText(debug_img, debug_info, (10, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            y_pos += 30
            cv2.putText(debug_img, f"æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“! è¯·å°è¯•å…¶ä»–ç±»åˆ«æˆ–é™ä½ç½®ä¿¡åº¦é˜ˆå€¼", 
                       (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            for i, common_class in enumerate(COMMON_CLASSES[:6]):
                y_pos += 30
                cv2.putText(debug_img, f"- {common_class}", 
                           (30, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            # ä¿å­˜ç©ºç»“æœåˆ°JSON
            json_path = os.path.join("results", f"separate_segmentation_{int(time.time())}.json")
            save_results_to_json([], json_path, "separate_segmentation")
            
            return debug_img[:, :, ::-1], json_path
        
        result_image = results[0].plot()
        cv2.putText(result_image, debug_info, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ä¿å­˜åˆ†å‰²ç»“æœåˆ°JSON
        json_path = os.path.join("results", f"separate_segmentation_{int(time.time())}.json")
        save_results_to_json(results, json_path, "separate_segmentation")
        
        return result_image[:, :, ::-1], json_path
        
    except Exception as e:
        print(f"YOLOEé”™è¯¯: {str(e)}")
        error_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
        cv2.putText(error_img, f"YOLOEé”™è¯¯: {str(e)}", (20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        y_pos = 100
        cv2.putText(error_img, "å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "1. è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œæ¨¡å‹å°†è‡ªåŠ¨ä¸‹è½½", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "2. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®: " + model_path, 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "3. ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç±»åˆ«åç§°", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "4. å°è¯•è¿™äº›å¸¸è§ç±»åˆ«:", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        for i, common_class in enumerate(COMMON_CLASSES[:5]):
            y_pos += 30
            cv2.putText(error_img, f"   - {common_class}", 
                       (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°JSON
        json_path = os.path.join("results", f"error_{int(time.time())}.json")
        error_data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "mode": "separate_segmentation",
            "error": str(e),
            "model_path": model_path
        }
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(error_data, f, ensure_ascii=False, indent=2)
        
        return error_img, json_path

# ==================== åº”ç”¨å…¥å£ ====================
def app():
    # ç¡®ä¿æ¨¡å‹å’Œç»“æœç›®å½•å­˜åœ¨
    os.makedirs("model", exist_ok=True)
    os.makedirs("results", exist_ok=True)
    os.makedirs("img", exist_ok=True)
    
    # ç¤ºä¾‹å›¾ç‰‡è·¯å¾„
    test_image_path = os.path.abspath("img/test.png")
    example_samples = []
    segmentation_examples = []
    separate_seg_examples = []
    
    # åªåœ¨ç¤ºä¾‹å›¾ç‰‡å­˜åœ¨æ—¶æ·»åŠ ç¤ºä¾‹
    if os.path.exists(test_image_path):
        example_samples.append([test_image_path, "YOLOv11n", 640, 0.25])
        segmentation_examples.append([test_image_path, "YOLOev11m-pf", 640, 0.25])
        separate_seg_examples.append([test_image_path, "YOLOev11m", 640, 0.25, "person,car"])
    
    # ä½¿ç”¨Softä¸»é¢˜æ„å»ºç•Œé¢
    with gr.Blocks(title="YOLOv11ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¯ YOLOv11 ç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²ç³»ç»Ÿ")
        gr.Markdown("ä¸Šä¼ å›¾åƒæˆ–è§†é¢‘è¿›è¡Œç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ï¼Œæ”¯æŒå‚æ•°è°ƒèŠ‚å’Œè‡ªå®šä¹‰ç±»åˆ«")
        
        with gr.Row():
            # å·¦ä¾§æ§åˆ¶é¢æ¿
            with gr.Column(scale=1):
                input_type = gr.Radio(
                    choices=["å›¾ç‰‡", "è§†é¢‘", "åˆ†å‰²", "å•ç‹¬åˆ†å‰²"],
                    value="å›¾ç‰‡",
                    label="è¾“å…¥ç±»å‹",
                    info="é€‰æ‹©å¤„ç†çš„åª’ä½“ç±»å‹å’Œä»»åŠ¡"
                )
                
                # å„ç§è¾“å…¥ç»„ä»¶
                with gr.Group(visible=True) as image_group:
                    image = gr.Image(label="è¾“å…¥å›¾åƒ", type="pil")
                
                with gr.Group(visible=False) as video_group:
                    video = gr.Video(label="è¾“å…¥è§†é¢‘")
                
                with gr.Group(visible=False) as segmentation_group:
                    segmentation_image = gr.Image(label="åˆ†å‰²è¾“å…¥å›¾åƒ", type="pil")
                
                with gr.Group(visible=False) as separate_seg_group:
                    separate_seg_image = gr.Image(label="å•ç‹¬åˆ†å‰²è¾“å…¥å›¾åƒ", type="pil")
                    classes = gr.Textbox(
                        label="æ£€æµ‹ç±»åˆ«",
                        placeholder="person,car,dog",
                        info="è¾“å…¥è¦æ£€æµ‹çš„ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¿…é¡»ä½¿ç”¨è‹±æ–‡é€—å·å’Œè‹±æ–‡åç§°"
                    )
                    gr.Markdown("""
                    ### å¸¸ç”¨ç±»åˆ«å‚è€ƒï¼š
                    - äººç‰©ï¼šperson
                    - äº¤é€šå·¥å…·ï¼šcar, bicycle, motorcycle, bus, truck
                    - åŠ¨ç‰©ï¼šdog, cat, bird, horse, sheep, cow
                    - ç‰©å“ï¼šbottle, cup, chair, couch, bed, dining table
                    """)
                
                with gr.Accordion("æ¨¡å‹å‚æ•°", open=False):
                    with gr.Group(visible=True) as yolo_model_group:
                        yolo_model = gr.Dropdown(
                            label="YOLOæ¨¡å‹é€‰æ‹©",
                            choices=yolo_models,
                            value="YOLOv11n",
                            info="é€‰æ‹©è¦ä½¿ç”¨çš„YOLOv11æ¨¡å‹ç‰ˆæœ¬"
                        )
                    
                    with gr.Group(visible=False) as yoloe_pf_model_group:
                        yoloe_pf_model = gr.Dropdown(
                            label="åˆ†å‰²æ¨¡å‹é€‰æ‹©",
                            choices=yoloe_pf_models,
                            value="YOLOev11m-pf",
                            info="é€‰æ‹©è¦ä½¿ç”¨çš„YOLOEåˆ†å‰²æ¨¡å‹"
                        )
                    
                    with gr.Group(visible=False) as yoloe_seg_model_group:
                        yoloe_seg_model = gr.Dropdown(
                            label="å•ç‹¬åˆ†å‰²æ¨¡å‹é€‰æ‹©",
                            choices=yoloe_seg_models,
                            value="YOLOev11m",
                            info="é€‰æ‹©è¦ä½¿ç”¨çš„YOLOEå•ç‹¬åˆ†å‰²æ¨¡å‹"
                        )
                    
                    image_size = gr.Slider(
                        minimum=320, maximum=1280, value=640, step=32,
                        label="å›¾åƒå°ºå¯¸",
                        info="æ›´å¤§çš„å°ºå¯¸é€šå¸¸èƒ½æé«˜ç²¾åº¦ï¼Œä½†ä¼šé™ä½é€Ÿåº¦"
                    )
                    
                    conf_threshold = gr.Slider(
                        minimum=0.0, maximum=1.0, value=0.25, step=0.05,
                        label="ç½®ä¿¡åº¦é˜ˆå€¼",
                        info="è°ƒæ•´æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œè¾ƒä½çš„å€¼ä¼šå¢åŠ æ£€æµ‹æ•°é‡ä½†å¯èƒ½å¢åŠ è¯¯æŠ¥"
                    )
                
                submit_btn = gr.Button("ğŸš€ å¼€å§‹æ£€æµ‹", variant="primary")
            
            # å³ä¾§ç»“æœå±•ç¤º
            with gr.Column(scale=2):
                output_image = gr.Image(label="æ£€æµ‹ç»“æœ", type="numpy", visible=True)
                output_video = gr.Video(label="æ£€æµ‹ç»“æœ", visible=False)
                output_segmentation = gr.Image(label="åˆ†å‰²ç»“æœ", type="numpy", visible=False)
                output_separate_seg = gr.Image(label="å•ç‹¬åˆ†å‰²ç»“æœ", type="numpy", visible=False)
                json_output = gr.File(label="æ£€æµ‹ç»“æœJSONæ–‡ä»¶")
        
        # ç¤ºä¾‹åŒºå— - å›¾ç‰‡æ£€æµ‹
        if example_samples:
            with gr.Accordion("å›¾ç‰‡æ£€æµ‹ç¤ºä¾‹", open=False, visible=True) as image_examples_ui:
                gr.Examples(
                    examples=example_samples,
                    inputs=[image, yolo_model, image_size, conf_threshold],
                    outputs=output_image,
                    fn=lambda img, model, size, conf: (
                        yolo_inference(img, None, model, size, conf)[0] 
                        if img is not None else np.zeros((100, 100, 3), dtype=np.uint8)
                    ),
                    cache_examples=True,
                    label="å›¾ç‰‡æ£€æµ‹ç¤ºä¾‹"
                )
        
        # ç¤ºä¾‹åŒºå— - åˆ†å‰²
        if segmentation_examples:
            with gr.Accordion("åˆ†å‰²ç¤ºä¾‹", open=False, visible=False) as segmentation_examples_ui:
                gr.Examples(
                    examples=segmentation_examples,
                    inputs=[segmentation_image, yoloe_pf_model, image_size, conf_threshold],
                    outputs=output_segmentation,
                    fn=lambda img, model, size, conf: (
                        yoloe_segmentation(img, model, size, conf)[0] 
                        if img is not None else np.zeros((100, 100, 3), dtype=np.uint8)
                    ),
                    cache_examples=True,
                    label="åˆ†å‰²ç¤ºä¾‹"
                )
        
        # ç¤ºä¾‹åŒºå— - å•ç‹¬åˆ†å‰²
        if separate_seg_examples:
            with gr.Accordion("å•ç‹¬åˆ†å‰²ç¤ºä¾‹", open=False, visible=False) as separate_seg_examples_ui:
                gr.Examples(
                    examples=separate_seg_examples,
                    inputs=[separate_seg_image, yoloe_seg_model, image_size, conf_threshold, classes],
                    outputs=output_separate_seg,
                    fn=lambda img, model, size, conf, classes: (
                        yoloe_separate_segmentation(img, model, size, conf, classes)[0] 
                        if img is not None else np.zeros((100, 100, 3), dtype=np.uint8)
                    ),
                    cache_examples=True,
                    label="å•ç‹¬åˆ†å‰²ç¤ºä¾‹"
                )
        
        # äº¤äº’é€»è¾‘
        def update_visibility(input_type):
            # æ›´æ–°è¾“å…¥ç»„
            image_group_visibility = input_type == "å›¾ç‰‡"
            video_group_visibility = input_type == "è§†é¢‘"
            segmentation_group_visibility = input_type == "åˆ†å‰²"
            separate_seg_group_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            # æ›´æ–°æ¨¡å‹é€‰æ‹©ç»„
            yolo_model_visibility = input_type in ["å›¾ç‰‡", "è§†é¢‘"]
            yoloe_pf_model_visibility = input_type == "åˆ†å‰²"
            yoloe_seg_model_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            # æ›´æ–°è¾“å‡ºæ˜¾ç¤º
            output_image_visibility = input_type == "å›¾ç‰‡"
            output_video_visibility = input_type == "è§†é¢‘"
            output_segmentation_visibility = input_type == "åˆ†å‰²"
            output_separate_seg_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            # æ›´æ–°ç¤ºä¾‹åŒºåŸŸæ˜¾ç¤º
            image_examples_visibility = input_type in ["å›¾ç‰‡", "è§†é¢‘"]
            segmentation_examples_visibility = input_type == "åˆ†å‰²"
            separate_seg_examples_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            return (
                # è¾“å…¥ç»„å¯è§æ€§
                gr.update(visible=image_group_visibility),
                gr.update(visible=video_group_visibility),
                gr.update(visible=segmentation_group_visibility),
                gr.update(visible=separate_seg_group_visibility),
                
                # æ¨¡å‹é€‰æ‹©ç»„å¯è§æ€§
                gr.update(visible=yolo_model_visibility),
                gr.update(visible=yoloe_pf_model_visibility),
                gr.update(visible=yoloe_seg_model_visibility),
                
                # è¾“å‡ºæ˜¾ç¤ºå¯è§æ€§
                gr.update(visible=output_image_visibility),
                gr.update(visible=output_video_visibility),
                gr.update(visible=output_segmentation_visibility),
                gr.update(visible=output_separate_seg_visibility),
                
                # ç¤ºä¾‹åŒºåŸŸå¯è§æ€§
                gr.update(visible=image_examples_visibility),
                gr.update(visible=segmentation_examples_visibility),
                gr.update(visible=separate_seg_examples_visibility),
            )

        input_type.change(
            fn=update_visibility,
            inputs=[input_type],
            outputs=[
                # è¾“å…¥ç»„
                image_group, video_group, segmentation_group, separate_seg_group,
                # æ¨¡å‹é€‰æ‹©ç»„
                yolo_model_group, yoloe_pf_model_group, yoloe_seg_model_group,
                # è¾“å‡ºæ˜¾ç¤º
                output_image, output_video, output_segmentation, output_separate_seg,
                # ç¤ºä¾‹åŒºåŸŸ
                image_examples_ui, segmentation_examples_ui, separate_seg_examples_ui,
            ],
        )

        def run_inference(input_type, 
                         image, video, segmentation_image, separate_seg_image, classes,
                         yolo_model, yoloe_pf_model, yoloe_seg_model,
                         image_size, conf_threshold):
            try:
                if input_type == "å›¾ç‰‡":
                    if image is None:
                        # å¤„ç†ç©ºè¾“å…¥
                        empty_img = np.zeros((400, 600, 3), dtype=np.uint8)
                        cv2.putText(empty_img, "è¯·ä¸Šä¼ å›¾ç‰‡", (50, 50), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        return empty_img, None, None, None, None
                        
                    output_img, _, json_path = yolo_inference(image, None, yolo_model, image_size, conf_threshold)
                    return output_img, None, None, None, json_path
                elif input_type == "è§†é¢‘":
                    if video is None:
                        # å¤„ç†ç©ºè¾“å…¥
                        empty_img = np.zeros((400, 600, 3), dtype=np.uint8)
                        cv2.putText(empty_img, "è¯·ä¸Šä¼ è§†é¢‘", (50, 50), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        return empty_img, None, None, None, None
                        
                    _, output_vid, json_path = yolo_inference(None, video, yolo_model, image_size, conf_threshold)
                    return None, output_vid, None, None, json_path
                elif input_type == "åˆ†å‰²":
                    if segmentation_image is None:
                        # å¤„ç†ç©ºè¾“å…¥
                        empty_img = np.zeros((400, 600, 3), dtype=np.uint8)
                        cv2.putText(empty_img, "è¯·ä¸Šä¼ å›¾ç‰‡", (50, 50), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        return None, None, empty_img, None, None
                        
                    seg_result, json_path = yoloe_segmentation(segmentation_image, yoloe_pf_model, image_size, conf_threshold)
                    return None, None, seg_result, None, json_path
                elif input_type == "å•ç‹¬åˆ†å‰²":
                    if separate_seg_image is None:
                        # å¤„ç†ç©ºè¾“å…¥
                        empty_img = np.zeros((400, 600, 3), dtype=np.uint8)
                        cv2.putText(empty_img, "è¯·ä¸Šä¼ å›¾ç‰‡", (50, 50), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        return None, None, None, empty_img, None
                        
                    separate_seg_result, json_path = yoloe_separate_segmentation(separate_seg_image, yoloe_seg_model, image_size, conf_threshold, classes)
                    return None, None, None, separate_seg_result, json_path
            except Exception as e:
                # å¤„ç†è¿è¡Œæ—¶é”™è¯¯
                print(f"è¿è¡Œé”™è¯¯: {str(e)}")
                error_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
                cv2.putText(error_img, f"è¿è¡Œé”™è¯¯: {str(e)}", (20, 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # æ ¹æ®å½“å‰æ¨¡å¼å†³å®šè¿”å›å“ªä¸ªè¾“å‡º
                if input_type == "å›¾ç‰‡":
                    return error_img, None, None, None, None
                elif input_type == "è§†é¢‘":
                    return error_img, None, None, None, None
                elif input_type == "åˆ†å‰²":
                    return None, None, error_img, None, None
                elif input_type == "å•ç‹¬åˆ†å‰²":
                    return None, None, None, error_img, None

        submit_btn.click(
            fn=run_inference,
            inputs=[
                input_type, 
                image, video, segmentation_image, separate_seg_image, classes,
                yolo_model, yoloe_pf_model, yoloe_seg_model,
                image_size, conf_threshold
            ],
            outputs=[output_image, output_video, output_segmentation, output_separate_seg, json_output],
        )
        
        return demo

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == '__main__':
    demo = app()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False
    )
