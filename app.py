import gradio as gr
import cv2
import tempfile
from ultralytics import YOLO, YOLOE
import os
import numpy as np
import time

# æ˜ å°„æ¨¡å‹åç§°å’Œæ¨¡å‹æ–‡ä»¶è·¯å¾„
model_mapping = {
    # æ™®é€šYOLOæ¨¡å‹ - ç”¨äºå›¾ç‰‡/è§†é¢‘æ¨¡å¼
    "YOLOv11n": "model/yolo11n.pt",
    "YOLOv11m": "model/yolo11m.pt",
    # YOLOEæ¨¡å‹ - ç”¨äºåˆ†å‰²æ¨¡å¼
    "YOLOev11m-pf": "model/yoloe-11m-seg-pf.pt",
    # å•ç‹¬åˆ†å‰²æ¨¡å¼ç”¨çš„æ¨¡å‹
    "YOLOev11m": "model/yoloe-11m-seg.pt",
}

# YOLOEå¸¸è§ç±»åˆ«åç§° (ä»COCOæ•°æ®é›†)
COMMON_CLASSES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
    "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse",
    "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie",
    "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove",
    "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon",
    "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut",
    "cake", "chair", "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse"
]

# ä¸ºä¸åŒæ¨¡å¼å‡†å¤‡ä¸åŒçš„æ¨¡å‹é€‰é¡¹
yolo_models = ["YOLOv11n", "YOLOv11m"]
yoloe_pf_models = ["YOLOev11m-pf"]
yoloe_seg_models = ["YOLOev11m"]

def yolo_inference(image, video, model_name, image_size, conf_threshold):
    model_path = model_mapping[model_name]
    model = YOLO(model_path)
    if image is not None:
        results = model.predict(source=image, imgsz=image_size, conf=conf_threshold)
        annotated_image = results[0].plot()
        return annotated_image[:, :, ::-1], None
    elif video is not None:
        video_path = tempfile.mktemp(suffix=".mp4")
        with open(video_path, "wb") as f:
            with open(video, "rb") as g:
                f.write(g.read())

        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        output_video_path = tempfile.mktemp(suffix=".mp4")
        # ä½¿ç”¨mp4æ ¼å¼å’ŒH.264ç¼–ç å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            results = model.predict(source=frame, imgsz=image_size, conf=conf_threshold)
            annotated_frame = results[0].plot()
            out.write(annotated_frame)

        cap.release()
        out.release()

        # ç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
        time.sleep(1)
        
        if os.path.exists(output_video_path) and os.path.getsize(output_video_path) > 0:
            return None, output_video_path
        else:
            return None, None

def yoloe_segmentation(image, model_name, image_size, conf_threshold, classes=None):
    """æ™®é€šåˆ†å‰²æ¨¡å¼ - ä½¿ç”¨YOLOev11-pfæ¨¡å‹"""
    model_path = model_mapping[model_name]
    model = YOLOE(model_path)
    
    # æ‰§è¡Œé¢„æµ‹
    results = model.predict(source=image, imgsz=image_size, conf=conf_threshold)
    annotated_image = results[0].plot()
    return annotated_image[:, :, ::-1]

def yoloe_separate_segmentation(image, model_name, image_size, conf_threshold, classes):
    """å•ç‹¬åˆ†å‰²æ¨¡å¼ - ä½¿ç”¨YOLOev11æ¨¡å‹ï¼Œä½¿ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œå¼€æ”¾è¯æ±‡æ£€æµ‹å’Œåˆ†å‰²"""
    model_path = model_mapping[model_name]
    
    try:
        # åˆå§‹åŒ–YOLOEæ¨¡å‹ - ä¸éœ€è¦æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ŒYOLOEä¼šè‡ªåŠ¨ä¸‹è½½
        print(f"æ­£åœ¨åŠ è½½YOLOEæ¨¡å‹: {model_name}ï¼Œè·¯å¾„: {model_path}")
        model = YOLOE(model_path)
        print(f"YOLOEæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # è®¾ç½®åˆ†ç±»ç±»åˆ«ï¼ˆæ–‡æœ¬æç¤ºï¼‰
        if not classes or not classes.strip():
            # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥ç±»åˆ«ï¼Œä½¿ç”¨ä¸€ä¸ªé»˜è®¤ç±»åˆ«åˆ—è¡¨
            class_list = ["person", "car", "dog", "cat", "chair", "table", "bottle"]
            debug_info = "ä½¿ç”¨é»˜è®¤ç±»åˆ«: " + ", ".join(class_list)
        else:
            # å¤„ç†ç”¨æˆ·è¾“å…¥çš„ç±»åˆ«
            class_list = [c.strip() for c in classes.split(',') if c.strip()]
            if not class_list:
                class_list = ["person", "car", "dog", "cat"]
            debug_info = "ä½¿ç”¨æ–‡æœ¬æç¤ºæ£€æµ‹ç±»åˆ«: " + ", ".join(class_list)
        
        # è®¾ç½®æ–‡æœ¬æç¤º - è¿™æ˜¯YOLOEçš„æ ¸å¿ƒåŠŸèƒ½
        text_embeddings = model.get_text_pe(class_list)
        model.set_classes(class_list, text_embeddings)
        
        # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ä»¥å¢åŠ æ£€æµ‹å‡ ç‡
        actual_conf = min(conf_threshold, 0.15)
        
        # æ‰§è¡Œé¢„æµ‹
        results = model.predict(source=image, imgsz=image_size, conf=actual_conf, verbose=True)
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹ç»“æœï¼Œè¿”å›è°ƒè¯•ä¿¡æ¯
        if len(results[0].boxes) == 0:
            debug_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
            y_pos = 30
            cv2.putText(debug_img, debug_info, (10, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            y_pos += 30
            cv2.putText(debug_img, f"æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“! è¯·å°è¯•å…¶ä»–ç±»åˆ«æˆ–é™ä½ç½®ä¿¡åº¦é˜ˆå€¼", 
                       (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            for i, common_class in enumerate(COMMON_CLASSES[:6]):
                y_pos += 30
                cv2.putText(debug_img, f"- {common_class}", 
                           (30, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
            return debug_img[:, :, ::-1]
        
        # YOLOEè‡ªå¸¦çš„ç»“æœå¯è§†åŒ–åŒ…å«è¾¹æ¡†å’Œåˆ†å‰²æ©ç 
        # ä½¿ç”¨plot()æ–¹æ³•è·å–å¸¦æœ‰è¾¹æ¡†å’Œæ©ç çš„å®Œæ•´å¯è§†åŒ–ç»“æœ
        result_image = results[0].plot()
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        cv2.putText(result_image, debug_info, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        return result_image[:, :, ::-1]
        
    except Exception as e:
        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        print(f"YOLOEé”™è¯¯: {str(e)}")
        error_img = np.ones((400, 600, 3), dtype=np.uint8) * 255
        cv2.putText(error_img, f"YOLOEé”™è¯¯: {str(e)}", (20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # æç¤ºå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
        y_pos = 100
        cv2.putText(error_img, "å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "1. è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œæ¨¡å‹å°†è‡ªåŠ¨ä¸‹è½½", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "2. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®: " + model_path, 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "3. ç¡®ä¿ä½¿ç”¨è‹±æ–‡ç±»åˆ«åç§°", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_pos += 30
        cv2.putText(error_img, "4. å°è¯•è¿™äº›å¸¸è§ç±»åˆ«:", 
                   (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        for i, common_class in enumerate(COMMON_CLASSES[:5]):
            y_pos += 30
            cv2.putText(error_img, f"   - {common_class}", 
                       (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        return error_img

# ==================== åº”ç”¨å…¥å£ ====================
def app():
    # ç¤ºä¾‹å›¾ç‰‡è·¯å¾„
    test_image_path = os.path.abspath("img/test.png")
    example_samples = []
    segmentation_examples = []
    separate_seg_examples = []
    
    if os.path.exists(test_image_path):
        example_samples.append([test_image_path, "YOLOv11n", 640, 0.25])
        segmentation_examples.append([test_image_path, "YOLOev11m-pf", 640, 0.25])
        separate_seg_examples.append([test_image_path, "YOLOev11m", 640, 0.25, "person,car"])
    
    # ä½¿ç”¨Softä¸»é¢˜æ„å»ºç•Œé¢
    with gr.Blocks(title="YOLOv11ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¯ YOLOv11 ç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²ç³»ç»Ÿ")
        gr.Markdown("ä¸Šä¼ å›¾åƒæˆ–è§†é¢‘è¿›è¡Œç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ï¼Œæ”¯æŒå‚æ•°è°ƒèŠ‚å’Œè‡ªå®šä¹‰ç±»åˆ«")
        
        with gr.Row():
            # å·¦ä¾§æ§åˆ¶é¢æ¿
            with gr.Column(scale=1):
                input_type = gr.Radio(
                    choices=["å›¾ç‰‡", "è§†é¢‘", "åˆ†å‰²", "å•ç‹¬åˆ†å‰²"],
                    value="å›¾ç‰‡",
                    label="è¾“å…¥ç±»å‹",
                    info="é€‰æ‹©å¤„ç†çš„åª’ä½“ç±»å‹å’Œä»»åŠ¡"
                )
                
                # å„ç§è¾“å…¥ç»„ä»¶
                with gr.Group(visible=True) as image_group:
                    image = gr.Image(label="è¾“å…¥å›¾åƒ", type="pil")
                
                with gr.Group(visible=False) as video_group:
                    video = gr.Video(label="è¾“å…¥è§†é¢‘")
                
                with gr.Group(visible=False) as segmentation_group:
                    segmentation_image = gr.Image(label="åˆ†å‰²è¾“å…¥å›¾åƒ", type="pil")
                
                with gr.Group(visible=False) as separate_seg_group:
                    separate_seg_image = gr.Image(label="å•ç‹¬åˆ†å‰²è¾“å…¥å›¾åƒ", type="pil")
                    classes = gr.Textbox(
                        label="æ£€æµ‹ç±»åˆ«",
                        placeholder="person,car,dog",
                        info="è¾“å…¥è¦æ£€æµ‹çš„ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¿…é¡»ä½¿ç”¨è‹±æ–‡é€—å·å’Œè‹±æ–‡åç§°"
                    )
                    gr.Markdown("""
                    ### å¸¸ç”¨ç±»åˆ«å‚è€ƒï¼š
                    - äººç‰©ï¼šperson
                    - äº¤é€šå·¥å…·ï¼šcar, bicycle, motorcycle, bus, truck
                    - åŠ¨ç‰©ï¼šdog, cat, bird, horse, sheep, cow
                    - ç‰©å“ï¼šbottle, cup, chair, couch, bed, dining table
                    """)
                
                with gr.Accordion("æ¨¡å‹å‚æ•°", open=False):
                    with gr.Group(visible=True) as yolo_model_group:
                        yolo_model = gr.Dropdown(
                            label="YOLOæ¨¡å‹é€‰æ‹©",
                            choices=yolo_models,
                            value="YOLOv11n",
                            info="é€‰æ‹©è¦ä½¿ç”¨çš„YOLOv11æ¨¡å‹ç‰ˆæœ¬"
                        )
                    
                    with gr.Group(visible=False) as yoloe_pf_model_group:
                        yoloe_pf_model = gr.Dropdown(
                            label="åˆ†å‰²æ¨¡å‹é€‰æ‹©",
                            choices=yoloe_pf_models,
                            value="YOLOev11m-pf",
                            info="é€‰æ‹©è¦ä½¿ç”¨çš„YOLOEåˆ†å‰²æ¨¡å‹"
                        )
                    
                    with gr.Group(visible=False) as yoloe_seg_model_group:
                        yoloe_seg_model = gr.Dropdown(
                            label="å•ç‹¬åˆ†å‰²æ¨¡å‹é€‰æ‹©",
                            choices=yoloe_seg_models,
                            value="YOLOev11m",
                            info="é€‰æ‹©è¦ä½¿ç”¨çš„YOLOEå•ç‹¬åˆ†å‰²æ¨¡å‹"
                        )
                    
                    image_size = gr.Slider(
                        minimum=320, maximum=1280, value=640, step=32,
                        label="å›¾åƒå°ºå¯¸",
                        info="æ›´å¤§çš„å°ºå¯¸é€šå¸¸èƒ½æé«˜ç²¾åº¦ï¼Œä½†ä¼šé™ä½é€Ÿåº¦"
                    )
                    
                    conf_threshold = gr.Slider(
                        minimum=0.0, maximum=1.0, value=0.25, step=0.05,
                        label="ç½®ä¿¡åº¦é˜ˆå€¼",
                        info="è°ƒæ•´æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œè¾ƒä½çš„å€¼ä¼šå¢åŠ æ£€æµ‹æ•°é‡ä½†å¯èƒ½å¢åŠ è¯¯æŠ¥"
                    )
                
                submit_btn = gr.Button("ğŸš€ å¼€å§‹æ£€æµ‹", variant="primary")
            
            # å³ä¾§ç»“æœå±•ç¤º
            with gr.Column(scale=2):
                output_image = gr.Image(label="æ£€æµ‹ç»“æœ", type="numpy", visible=True)
                output_video = gr.Video(label="æ£€æµ‹ç»“æœ", visible=False)
                output_segmentation = gr.Image(label="åˆ†å‰²ç»“æœ", type="numpy", visible=False)
                output_separate_seg = gr.Image(label="å•ç‹¬åˆ†å‰²ç»“æœ", type="numpy", visible=False)
        
        # ç¤ºä¾‹åŒºå— - å›¾ç‰‡æ£€æµ‹
        if example_samples:
            with gr.Accordion("å›¾ç‰‡æ£€æµ‹ç¤ºä¾‹", open=False, visible=True) as image_examples_ui:
                gr.Examples(
                    examples=example_samples,
                    inputs=[image, yolo_model, image_size, conf_threshold],
                    outputs=output_image,
                    fn=lambda img, model, size, conf: yolo_inference(img, None, model, size, conf)[0],
                    cache_examples=True,
                    label="å›¾ç‰‡æ£€æµ‹ç¤ºä¾‹"
                )
        
        # ç¤ºä¾‹åŒºå— - åˆ†å‰²
        if segmentation_examples:
            with gr.Accordion("åˆ†å‰²ç¤ºä¾‹", open=False, visible=False) as segmentation_examples_ui:
                gr.Examples(
                    examples=segmentation_examples,
                    inputs=[segmentation_image, yoloe_pf_model, image_size, conf_threshold],
                    outputs=output_segmentation,
                    fn=yoloe_segmentation,
                    cache_examples=True,
                    label="åˆ†å‰²ç¤ºä¾‹"
                )
        
        # ç¤ºä¾‹åŒºå— - å•ç‹¬åˆ†å‰²
        if separate_seg_examples:
            with gr.Accordion("å•ç‹¬åˆ†å‰²ç¤ºä¾‹", open=False, visible=False) as separate_seg_examples_ui:
                gr.Examples(
                    examples=separate_seg_examples,
                    inputs=[separate_seg_image, yoloe_seg_model, image_size, conf_threshold, classes],
                    outputs=output_separate_seg,
                    fn=yoloe_separate_segmentation,
                    cache_examples=True,
                    label="å•ç‹¬åˆ†å‰²ç¤ºä¾‹"
                )
        
        # äº¤äº’é€»è¾‘
        def update_visibility(input_type):
            # æ›´æ–°è¾“å…¥ç»„
            image_group_visibility = input_type == "å›¾ç‰‡"
            video_group_visibility = input_type == "è§†é¢‘"
            segmentation_group_visibility = input_type == "åˆ†å‰²"
            separate_seg_group_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            # æ›´æ–°æ¨¡å‹é€‰æ‹©ç»„
            yolo_model_visibility = input_type in ["å›¾ç‰‡", "è§†é¢‘"]
            yoloe_pf_model_visibility = input_type == "åˆ†å‰²"
            yoloe_seg_model_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            # æ›´æ–°è¾“å‡ºæ˜¾ç¤º
            output_image_visibility = input_type == "å›¾ç‰‡"
            output_video_visibility = input_type == "è§†é¢‘"
            output_segmentation_visibility = input_type == "åˆ†å‰²"
            output_separate_seg_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            # æ›´æ–°ç¤ºä¾‹åŒºåŸŸæ˜¾ç¤º
            image_examples_visibility = input_type in ["å›¾ç‰‡", "è§†é¢‘"]
            segmentation_examples_visibility = input_type == "åˆ†å‰²"
            separate_seg_examples_visibility = input_type == "å•ç‹¬åˆ†å‰²"
            
            return (
                # è¾“å…¥ç»„å¯è§æ€§
                gr.update(visible=image_group_visibility),
                gr.update(visible=video_group_visibility),
                gr.update(visible=segmentation_group_visibility),
                gr.update(visible=separate_seg_group_visibility),
                
                # æ¨¡å‹é€‰æ‹©ç»„å¯è§æ€§
                gr.update(visible=yolo_model_visibility),
                gr.update(visible=yoloe_pf_model_visibility),
                gr.update(visible=yoloe_seg_model_visibility),
                
                # è¾“å‡ºæ˜¾ç¤ºå¯è§æ€§
                gr.update(visible=output_image_visibility),
                gr.update(visible=output_video_visibility),
                gr.update(visible=output_segmentation_visibility),
                gr.update(visible=output_separate_seg_visibility),
                
                # ç¤ºä¾‹åŒºåŸŸå¯è§æ€§
                gr.update(visible=image_examples_visibility),
                gr.update(visible=segmentation_examples_visibility),
                gr.update(visible=separate_seg_examples_visibility),
            )

        input_type.change(
            fn=update_visibility,
            inputs=[input_type],
            outputs=[
                # è¾“å…¥ç»„
                image_group, video_group, segmentation_group, separate_seg_group,
                # æ¨¡å‹é€‰æ‹©ç»„
                yolo_model_group, yoloe_pf_model_group, yoloe_seg_model_group,
                # è¾“å‡ºæ˜¾ç¤º
                output_image, output_video, output_segmentation, output_separate_seg,
                # ç¤ºä¾‹åŒºåŸŸ
                image_examples_ui, segmentation_examples_ui, separate_seg_examples_ui,
            ],
        )

        def run_inference(input_type, 
                         image, video, segmentation_image, separate_seg_image, classes,
                         yolo_model, yoloe_pf_model, yoloe_seg_model,
                         image_size, conf_threshold):
            if input_type == "å›¾ç‰‡":
                output_img, _ = yolo_inference(image, None, yolo_model, image_size, conf_threshold)
                return output_img, None, None, None
            elif input_type == "è§†é¢‘":
                _, output_vid = yolo_inference(None, video, yolo_model, image_size, conf_threshold)
                return None, output_vid, None, None
            elif input_type == "åˆ†å‰²":
                seg_result = yoloe_segmentation(segmentation_image, yoloe_pf_model, image_size, conf_threshold)
                return None, None, seg_result, None
            elif input_type == "å•ç‹¬åˆ†å‰²":
                separate_seg_result = yoloe_separate_segmentation(separate_seg_image, yoloe_seg_model, image_size, conf_threshold, classes)
                return None, None, None, separate_seg_result

        submit_btn.click(
            fn=run_inference,
            inputs=[
                input_type, 
                image, video, segmentation_image, separate_seg_image, classes,
                yolo_model, yoloe_pf_model, yoloe_seg_model,
                image_size, conf_threshold
            ],
            outputs=[output_image, output_video, output_segmentation, output_separate_seg],
        )
        
        return demo

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == '__main__':
    demo = app()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False
    )
