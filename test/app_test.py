import gradio as gr
import cv2
import tempfile
from ultralytics import YOLO
import os
import numpy as np

# æ˜ å°„æ¨¡å‹åç§°å’Œæ¨¡å‹æ–‡ä»¶è·¯å¾„
model_mapping = {
    "YOLOv11n": "model/yolo11n.pt"
}

def yolo_inference(image, video, model_name, image_size, conf_threshold):
    model_path = model_mapping[model_name]  # ä»æ˜ å°„ä¸­è·å–æ¨¡å‹è·¯å¾„
    model = YOLO(model_path)  # ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
    if image is not None:
        results = model.predict(source=image, imgsz=image_size, conf=conf_threshold)
        annotated_image = results[0].plot()
        return annotated_image[:, :, ::-1], None
    elif video is not None:
        video_path = tempfile.mktemp(suffix=".webm")
        with open(video_path, "wb") as f:
            with open(video, "rb") as g:
                f.write(g.read())

        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        output_video_path = tempfile.mktemp(suffix=".webm")
        out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'vp80'), fps, (frame_width, frame_height))

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            results = model.predict(source=frame, imgsz=image_size, conf=conf_threshold)
            annotated_frame = results[0].plot()
            out.write(annotated_frame)

        cap.release()
        out.release()

        return None, output_video_path


def yolo_inference_for_examples(image, model_name, image_size, conf_threshold):
    annotated_image, _ = yolo_inference(image, None, model_name, image_size, conf_threshold)
    return annotated_image


# ==================== åº”ç”¨å…¥å£ ====================
def app():
    # ç¤ºä¾‹å›¾ç‰‡è·¯å¾„
    test_image_path = os.path.abspath("img/test.png")
    example_samples = []
    if os.path.exists(test_image_path):
        example_samples.append([test_image_path, "YOLOv11n", 640, 0.25])
    
    # ä½¿ç”¨Softä¸»é¢˜æ„å»ºç•Œé¢
    with gr.Blocks(title="YOLOv11ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¯ YOLOv11 ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ")
        gr.Markdown("ä¸Šä¼ å›¾åƒæˆ–è§†é¢‘è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œæ”¯æŒå‚æ•°è°ƒèŠ‚")
        
        with gr.Row():
            # å·¦ä¾§æ§åˆ¶é¢æ¿
            with gr.Column(scale=1):
                input_type = gr.Radio(
                    choices=["å›¾ç‰‡", "è§†é¢‘"],
                    value="å›¾ç‰‡",
                    label="è¾“å…¥ç±»å‹",
                    info="é€‰æ‹©å¤„ç†çš„åª’ä½“ç±»å‹"
                )
                
                image = gr.Image(label="è¾“å…¥å›¾åƒ", type="pil", visible=True)
                video = gr.Video(label="è¾“å…¥è§†é¢‘", visible=False)
                
                with gr.Accordion("æ¨¡å‹å‚æ•°", open=False):
                    model_name = gr.Dropdown(
                        label="æ¨¡å‹é€‰æ‹©",
                        choices=list(model_mapping.keys()),
                        value="YOLOv11n",
                        info="é€‰æ‹©è¦ä½¿ç”¨çš„YOLOv11æ¨¡å‹ç‰ˆæœ¬"
                    )
                    
                    image_size = gr.Slider(
                        minimum=320, maximum=1280, value=640, step=32,
                        label="å›¾åƒå°ºå¯¸",
                        info="æ›´å¤§çš„å°ºå¯¸é€šå¸¸èƒ½æé«˜ç²¾åº¦ï¼Œä½†ä¼šé™ä½é€Ÿåº¦"
                    )
                    
                    conf_threshold = gr.Slider(
                        minimum=0.0, maximum=1.0, value=0.25, step=0.05,
                        label="ç½®ä¿¡åº¦é˜ˆå€¼",
                        info="è°ƒæ•´æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œè¾ƒä½çš„å€¼ä¼šå¢åŠ æ£€æµ‹æ•°é‡ä½†å¯èƒ½å¢åŠ è¯¯æŠ¥"
                    )
                
                submit_btn = gr.Button("ğŸš€ å¼€å§‹æ£€æµ‹", variant="primary")
            
            # å³ä¾§ç»“æœå±•ç¤º
            with gr.Column(scale=2):
                output_image = gr.Image(label="æ£€æµ‹ç»“æœ", type="numpy", visible=True)
                output_video = gr.Video(label="æ£€æµ‹ç»“æœ", visible=False)
        
        # ç¤ºä¾‹åŒºå—
        if example_samples:
            gr.Examples(
                examples=example_samples,
                inputs=[image, model_name, image_size, conf_threshold],
                outputs=output_image,
                fn=yolo_inference_for_examples,
                cache_examples=True,
                label="å¿«é€Ÿç¤ºä¾‹"
            )
        
        # äº¤äº’é€»è¾‘
        def update_visibility(input_type):
            image_visibility = input_type == "å›¾ç‰‡"
            video_visibility = input_type == "è§†é¢‘"
            
            return (
                gr.update(visible=image_visibility),
                gr.update(visible=video_visibility),
                gr.update(visible=image_visibility),
                gr.update(visible=video_visibility),
            )

        input_type.change(
            fn=update_visibility,
            inputs=[input_type],
            outputs=[image, video, output_image, output_video],
        )

        def run_inference(image, video, model_name, image_size, conf_threshold, input_type):
            if input_type == "å›¾ç‰‡":
                return yolo_inference(image, None, model_name, image_size, conf_threshold)
            else:
                return yolo_inference(None, video, model_name, image_size, conf_threshold)

        submit_btn.click(
            fn=run_inference,
            inputs=[image, video, model_name, image_size, conf_threshold, input_type],
            outputs=[output_image, output_video],
        )
        
        return demo

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == '__main__':
    demo = app()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False
    )
